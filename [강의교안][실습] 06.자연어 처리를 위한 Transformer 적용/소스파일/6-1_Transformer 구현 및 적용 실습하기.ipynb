{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_1_Transformer_IMDB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-qUz1F_5rif",
        "outputId": "ba6cbe60-80f4-455f-c959-52d3a4a30f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.7-py3-none-any.whl (526 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 40 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 71 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 92 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 337 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 348 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 358 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 368 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 378 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 389 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 399 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 409 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 419 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 430 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 440 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 450 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 460 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 471 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 481 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 491 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 501 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 512 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 522 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 526 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 36.3 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 33.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 62.8 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.8)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=f652e74c46824edc1d2e71214c24eaa0247610c2b2438b7244b3ff0336aadded\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.7 torchmetrics-0.6.2 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "kvraC7sZ5sQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d: int, heads: int=8):\n",
        "        super().__init__()\n",
        "        self.k, self.h = d, heads\n",
        "        \n",
        "        self.WQ = nn.Linear(d, d * heads, bias=False)\n",
        "        self.WK = nn.Linear(d, d * heads, bias=False)\n",
        "        self.WV = nn.Linear(d, d * heads, bias=False)\n",
        "        \n",
        "        self.unifyheads = nn.Linear(heads * d, d)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        b, l, d = x.size()\n",
        "        h = self.h\n",
        "        \n",
        "        queries = self.WQ(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        keys = self.WK(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        values = self.WV(x).view(b, l, h, d).transpose(1, 2).contiguous().view(b * h, l, d)\n",
        "        \n",
        "        w_prime = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(d)\n",
        "        w = F.softmax(w_prime, dim=-1)  \n",
        "        \n",
        "        out = torch.bmm(w, values).view(b, h, l, d)\n",
        "        \n",
        "        out = out.transpose(1, 2).contiguous().view(b, l, h * d)\n",
        "        \n",
        "        return self.unifyheads(out)"
      ],
      "metadata": {
        "id": "840QlP9E57MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d: int, heads: int=8, n_mlp: int=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = SelfAttention(d, heads=heads)\n",
        "        self.norm1 = nn.LayerNorm(d)\n",
        "        self.norm2 = nn.LayerNorm(d)\n",
        "        \n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d, n_mlp*d),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_mlp*d, d)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_prime = self.attention(x)\n",
        "        x = self.norm1(x_prime + x)\n",
        "        \n",
        "        x_prime = self.ff(x)\n",
        "        return self.norm2(x_prime + x)"
      ],
      "metadata": {
        "id": "dedlkVN85-bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class IMDBDataLoader(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, batch_size: int):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def setup(self, num_words: int, max_seq_len: int):\n",
        "        (self.x_train, self.y_train), (self.x_test, self.y_test) = imdb.load_data(\n",
        "            num_words=num_words, \n",
        "            maxlen=max_seq_len\n",
        "        )\n",
        "        \n",
        "        self.word2idx = dict(\n",
        "            **{k: v+3 for k, v in imdb.get_word_index().items()},\n",
        "            **{'<PAD>': 0, '<START>': 1, '<UNK>': 2, '<UNUSED>': 3,\n",
        "              },\n",
        "        )\n",
        "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
        "        \n",
        "        self.x_train = pad_sequences(self.x_train, maxlen=max_seq_len, value = 0.0)\n",
        "        self.x_test = pad_sequences(self.x_test, maxlen=max_seq_len, value = 0.0)\n",
        "        \n",
        "    \n",
        "    def example(self):\n",
        "        idx = np.random.randint(0, len(self.x_train))\n",
        "        x, y = self.x_train[idx], self.y_train[idx]\n",
        "        review = ' '.join(self.idx2word[token_id] for token_id in x if token_id > 1)\n",
        "        sentiment = 'POSITIVE' if y else 'NEGATIVE'\n",
        "        return f'Review : {review}\\nSentiment: {sentiment}'\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_train), \n",
        "                                torch.LongTensor(self.y_train))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "                                \n",
        "    def test_dataloader(self):\n",
        "        dataset = TensorDataset(torch.LongTensor(self.x_test), \n",
        "                                torch.LongTensor(self.y_test))\n",
        "        return DataLoader(dataset, self.batch_size)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self.test_dataloader()"
      ],
      "metadata": {
        "id": "gCsmisOq6BOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBTransformer(pl.LightningModule):\n",
        "    def __init__(self, d: int=128, heads: int=8, depth: int=6,\n",
        "                max_seq_len: int=512, num_tokens: int=30000, \n",
        "                num_classes: int=2, learning_rate: float=1e-4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.num_tokens = num_tokens\n",
        "        \n",
        "        self.token_emb = nn.Embedding(num_tokens, d)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d)\n",
        "        \n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(d=d, heads=heads) for _ in range(depth)]\n",
        "        )\n",
        "        \n",
        "        self.classification = nn.Linear(d, num_classes)\n",
        "        \n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = pl.metrics.Accuracy()\n",
        "        \n",
        "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
        "\n",
        "        b, l = x.size()\n",
        "        d = self.hparams.d\n",
        "\n",
        "        tokens = self.token_emb(x)\n",
        "        positions = self.pos_emb(torch.arange(l).to(self.device)).expand(b, l, d)\n",
        "        embeddings = tokens + positions\n",
        "\n",
        "        out = self.transformer_blocks(embeddings)\n",
        "\n",
        "        out = out.mean(dim=1)\n",
        "        out = self.classification(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "         return Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch      \n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        self.log('loss', loss, on_epoch=True, prog_bar=True)\n",
        "        self.log('acc', self.accuracy(logits, y), on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        \n",
        "        self.log('test_loss', loss, on_epoch=True)\n",
        "        self.log('test_acc', self.accuracy(logits, y), on_epoch=True,\n",
        "                 prog_bar=True)\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.test_step(batch, batch_idx)"
      ],
      "metadata": {
        "id": "_1mTXyuc6DUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORDS = 10000\n",
        "MAX_SEQ_LEN = 128\n",
        "EMBEDDING_DIM = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "imdb_data = IMDBDataLoader(batch_size=BATCH_SIZE)\n",
        "imdb_data.setup(num_words=NUM_WORDS, max_seq_len=MAX_SEQ_LEN)\n",
        "model = IMDBTransformer(d=EMBEDDING_DIM, max_seq_len=MAX_SEQ_LEN, num_tokens=NUM_WORDS)\n",
        "trainer = pl.Trainer(max_epochs=5,\n",
        "                     gpus=1)\n",
        "trainer.fit(model, imdb_data)\n",
        "_ = trainer.test()"
      ],
      "metadata": {
        "id": "5sa0jUe46PDU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}